# (APPENDIX) Apêndice {-} 

# Breve Resumo de Medida e Probabilidade {#medprob} 

Essa seção tem o objetivo de apresentar as ideias de probabilidade como uma medida e da integral de Lebesgue. Para maiores detalhes, ver @Ash00, @Billingsley86, @Shiryaev96 ou, para uma versão mais resumida, os Apêndices de  @Schervish12.


## Conceitos Básicos {#basprob}

+ $\Omega$: espaço amostral (um conjunto não vazio). 

+ $\mathcal{A}$: *$\sigma$-álgebra de subconjuntos* de $\Omega$, isto é,
    1. $\Omega \in \mathcal{A}$;
    2. $A \in \mathcal{A} \Longrightarrow A^{c} \in \mathcal{A}$;
    3. $\displaystyle A_1, A_2, \ldots \in \mathcal{A} \Longrightarrow \bigcup_{i\geq1} A_i \in \mathcal{A}$.

+ Os elementos de $\mathcal{A}$ são chamados de *eventos* e serão denotados por $A, B, C, \ldots, A_1, A_2, \ldots$

+ Uma coleção de eventos $A_1,A_2,\ldots$ forma uma *partição* de $\Omega$ se $A_i \cap A_j = \varnothing$, $\forall i \neq j$, e $\displaystyle \bigcup_{i=1}^{\infty} A_i = \Omega$.

+ $(\Omega, \mathcal{A})$: *espaço mensurável*.

+ Usualmente, denota-se a $\sigma$-álgebra gerada por um conjunto $\mathcal{C}$ como $\sigma(\mathcal{C})$. Por exemplo:
    + $\sigma(\Omega) = \{\varnothing,\Omega\}~~$ ($\sigma$-ágebra trivial);
    + Para $A \subset \Omega$, $\sigma(A) = \{\varnothing, A, A^c, \Omega\}$;
    + $\sigma(\mathbb{N}) = \mathcal{P}(\mathbb{N})~~$ (partes de $\mathbb{N}$, todos o subconjuntos de $\mathbb{N}$);
    + $\sigma\left(\left\{(-\infty,x): x \in \mathbb{R}\right\}\right) = \mathcal{B}\left(\mathbb{R}\right)~~$ (borelianos de $\mathbb{R}$)

$~$

**Definição:** A função $\mu: \mathcal{A} \longrightarrow \bar{\mathbb{R}}_+$ é uma *medida* se  
1. $\mu(\varnothing) = 0$;  
2. $\displaystyle A_1, A_2, \ldots \in \mathcal{A}$ com $A_i \bigcap A_j = \varnothing$ , $\forall i \neq j$ ,  $\displaystyle \mu\left(\bigcup_{i \geq 1} A_i\right) = \sum_{i \geq 1} \mu\left(A_i\right)$.

+ $(\Omega,\mathcal{A}, \mu)$ é chamado de *espaço de medida*.

$~$

> **Exemplo 1 (medida de contagem):** Seja $\Omega$ um conjunto não vazio e $A\subseteq \Omega$. Defina $\nu(A)=|A|$ como o número de elementos (cardinalidade) de $A$. Assim, $\nu(\Omega) > 0$, $\nu(\varnothing)=0$ e, se $(A_n)_{n \geq 1}$ é uma sequência de eventos disjuntos, então $\nu(\cup A_n) = \sum \nu(A_n)$. Note que $\nu(A)=\infty$ é possivel se $\Omega$ tem infinitos elementos.

$~$

> **Exemplo 2 (medida de Lebesgue):** Seja $\Omega=\mathbb{R}$ e $A\subseteq \Omega$ um intervalo. Se $A$ é limitado, defina $\lambda(A)$ como o comprimento do intervalo $A$. Se $A$ não é limitado, $\lambda(A)=\infty$. Note que $\lambda(\mathbb{R})=\infty$, $\lambda(\varnothing)=0$ e, se $A_1 \cap A_2 = \varnothing$ e $A_1 \cup A_2$ é um intervalo (ou uma união de intervalos disjuntos), então $\lambda(A_1 \cup A_2) = \lambda(A_1) + \lambda(A_2)$.

$~$

> **Exemplo 3:** Seja $f: \mathbb{R} \longrightarrow \mathbb{R}_+$ uma função contínua e não nula. Para cada intervalo $A$, defina $\displaystyle \mu(A) = \int_A f(x) dx = \int_{\mathbb{R}} \mathbb{I}_A(x) f(x) dx$. Então, $\mu(\mathbb{R})>0$, $\mu(\varnothing)=0$ e, se $A_1 \cap A_2 = \varnothing$ e $A_1 \cup A_2$ é um intervalo (ou uma união de intervalos disjuntos), então $\mu(A_1 \cup A_2) = \mu(A_1) + \mu(A_2)$.

$~$

+ Se $\mu(\Omega) < \infty$ dizemos que $\mu$ é uma *medida finita*. Se existe uma partição enumerável de $\Omega$, $A_1,A_2,\ldots$, tal que cada elemento da partição tem medida finita, $\mu(A_i)<\infty$, $\forall i$, dizemos que $\mu$ é uma *medida $\sigma$-finita*.

$~$


**Definição:**  $P: \mathcal{A} \longrightarrow [0,1]$ é uma **medida de probabilidade** se  
1. $P(\Omega) = 1$;  
2. $\displaystyle A_1, A_2, \ldots \in \mathcal{A}$ com $A_i \bigcap A_j = \varnothing$ , $\displaystyle P\left(\bigcup_{i \geq 1} A_i\right) = \sum_{i \geq 1} P\left(A_i\right)$.  
    
+ $(\Omega, \mathcal{A}, P)$: espaço de probabilidade

$~$

**Definição:** Seja $(\Omega,\mathcal{A})$ e $(\mathfrak{X},\mathcal{F})$ dois espaços mensuráveis. Uma função $X: \Omega \longrightarrow \mathfrak{X}$ é chamado de *quantidade aleatória* se é uma *função mensurável*, isto é, se $\forall B \in \mathcal{F}$, o evento $A = X^{-1}(B)$ $= \left\{\omega \in \Omega:~X(\omega)\in B\right\}$ pertence à $\sigma$-álgebra original $\mathcal{A}$.  
Se $\mathfrak{X} = \mathbb{R}$ e $\mathcal{F}=\mathcal{B}(\mathbb{R})$ ($\sigma$-álgebra de Borel), $X$ é chamada **variável aleatória** (v.a.).

+ Considere $(\Omega,\mathcal{A},P)$. A medida de probabilidade $P_X$ induzida por $X$ recebe o nome de *distribuição de $X$*. Se $B \in \mathcal{F}$ e $A = \{\omega \in \Omega :  X(\omega) \in B\} \in \mathcal{A}$, a medida induzida por $X$ é
$$P_X(B) = P_X\left(X \in B\right) = P\left(\{\omega \in \Omega :  X(\omega) \in B\}\right) = P(A)~.$$

+ A distribuição de $X$ é dita ser *discreta* se existe um conjunto enumerável $A \subseteq \mathfrak{X}$ tal que $P_X(A)=1$. A distribuição de $X$ é *contínua* se $P_X\left(\{x\}\right)=0$ para todo $x \in \mathfrak{X}$.

$~$

$~$ 

## Valor Esperado de $X$ (OU uma ideia da tal Integral de Lebesgue) {#lebesgue}

Por simplicidade, considere o espaço $\Big(\Omega = [0,1]~,~~ \mathcal{A} = \mathcal{B}\left([0,1]\right)~,~~ P=\lambda\Big)$. 

Seja $X: \Omega \longrightarrow \mathbb{R}_+$ uma variável aleatória discreta, assumindo valores não negativos $\mathfrak{X}=\{x_1,x_2,\ldots,x_k\}$ com probabilidades $\{p_1,p_2,\ldots,p_k\}$. Nos cursos básicos de probabilidade é visto que o *valor esperado* (ou *esperança*) de $X$ é $E[X] =$ $\sum x_i P(X=x_i) =$ $\sum x_i p_i$.

Podemos definir essa v.a. como

$X(\omega) = \left\{\begin{array}{ll} x_1, & \omega \in [0,p_1] = A_1 \\ 
  x_2, & \omega \in [p_1,p_1+p_2] = A_2 \\
  \vdots & \\
  x_j, & \omega \in \left[\displaystyle\sum_{i=1}^{j-1} p_i,\sum_{i=1}^{j} p_i\right] = A_j \\
  \vdots & \\
  x_k, & \omega \in [1-p_k,1] = A_k
\end{array}\right.$

```{r va_discreta, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
xgraf <- tibble(x=c(0,1),y=c(0,1)) %>% 
  ggplot(.,aes(x=x,y=y)) + #geom_line() +# labs(x = "w", y="X(w)") +
  scale_x_continuous(breaks=c(0,1/6,2/6,3/6,4/6,5/6,1),labels=c("0",expression(p[1]), expression(p[1]+p[2]), expression(p[1]+p[2]+p[3]), expression(ldots), expression(1-p[k]), "1"), limits=c(0,1), name=expression(omega)) +
  scale_y_continuous(breaks=c(0.2,0.4,0.7,0.9),labels=c(expression(x[1]),expression(x[3]),expression(x[k]),expression(x[2])),limits=c(0,1), name=expression(X(omega)))

dfv <- tibble(x1=c(0,1/6,2/6,3/6,5/6,1),x2=c(0,1/6,2/6,3/6,5/6,1),y1=c(0,0,0,0,0,0),y2=c(0.2,0.9,0.9,0.4,0.7,0.7))
dfh <- tibble(x1=c(0,1/6,2/6,5/6),x2=c(1/6,2/6,3/6,1),y1=c(0.2,0.9,0.4,0.7),y2=c(0.2,0.9,0.4,0.7))
xgraf + theme_bw() +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = dfv, linetype=2) +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = dfh, linetype=1, size=2)
```

Note que a medida $\lambda$ define uma distribuição uniforme no espaço $(\Omega,\mathcal{A})$. Assim, temos que 

+ $P_X(X=x_1)$ $=P\left(X^{-1}(x_1)\right)$ $=P\left(\{\omega \in \Omega : X(\omega)=x_1\}\right)$ $=P(A_1)$ $=\lambda\left([0,p_1]\right)$ $=p_1$,

+ $P_X(X=x_j)$ $=P\left(\{\omega \in \Omega : X(\omega)=x_j\}\right)$ $=\lambda\left(\left[\sum_{i=1}^{j-1} p_i,\sum_{i=1}^{j} p_i\right]\right)$ $=p_j ~,~$ $j \in \{2,\ldots,k\}$.

$~$

**Definição:** Uma função mensurável $X: \Omega \longrightarrow \mathbb{R}_+$ é dita *simples* se assumir um número finito de valores.

$~$

**Definição:** Considere um espaço de probabilidade $(\Omega, \mathcal{A}, P)$, $X:\Omega\longrightarrow \mathbb{R}_+$ v.a. assumindo valores $\{x_1,x_2,\ldots,x_k\}$ e $A_1,A_2,\ldots,A_k$ eventos disjuntos em $\mathcal{A}$. Seja $\displaystyle X(\omega) = \sum_{i=1}^{k} x_i ~\mathbb{I}_{A_i}(\omega)$, uma função simples com $A_i = X^{-1}(x_i)$, $i=1,\ldots,k$. A *integral de Lebesgue* de $X$ em relação à medida $P$ é 
$$E[X] = \int_\Omega X dP = \sum_{i=1}^{k} x_i P(A_i).$$

$~$

**Propriedades:** se $X, Y: \Omega \longrightarrow \mathbb{R}_+$ são funções simples, então

**1.** $\displaystyle\int_\Omega X dP \geq 0$;

**2.** $\displaystyle\int_\Omega cX dP = c\int_\Omega X dP$;

**3.** $\displaystyle\int_\Omega (X+Y) dP = \int_\Omega X dP + \int_\Omega Y dP$.

$~$

> **Demo 1.** Segue de $x_i \geq 0$  e $P(A_i) \geq 0$.  

> **Demo 2.**  
Para $X$ v.a. temos  
$X =\displaystyle \sum_{i=1}^kx_i~\mathbb{I}_{A_i}$ e $cX = \displaystyle\sum_{i=1}^k c~x_i ~\mathbb{I}_{A_i}$. Logo,  
$\displaystyle\int_\Omega cX~dP = \sum_{i=1}^k c~x_i~P(A_i)$ $=\displaystyle c\sum_{i=1}^kx_i P(A_i) = c\int_\Omega X dP$.  

> **Demo 3.**  
$X = \sum_{i=1}^kx_i~\mathbb{I}_{A_i}$ e $Y = \sum_{j=1}^ly_j~\mathbb{I}_{B_j}$.  
$X + Y$ $=\displaystyle \sum_{i=1}^k x_i ~\mathbb{I}_{A_i} + \sum_{j=1}^l y_j~\mathbb{I}_{B_j}$ $=\displaystyle\sum_{i=1}^k\sum_{j=1}^lx_i~\mathbb{I}_{A_i\cap B_j} + \sum_{i=1}^k\sum_{j=1}^ly_j~\mathbb{I}_{A_i\cap B_j}$ $=\displaystyle\sum_{i=1}^k\sum_{j=1}^l(x_i+y_j)~\mathbb{I}_{A_i\cap B_j}$.  
$\displaystyle \int_\Omega (X + Y) dP$ $=\displaystyle \sum_{i=1}^k\sum_{j=1}^l (x_i + y_j)P(A_i\cap B_j)$ $=\displaystyle\sum_{i=1}^k\sum_{j=1}^l x_iP(A_i\cap B_j) + \sum_{i=1}^k\sum_{j=1}^l y_jP(A_i\cap B_j)$ $=\displaystyle\sum_{i=1}^k x_i P(A_i) + \sum_{j=1}^l y_j P(B_j)$ $=\displaystyle\int_\Omega X dP + \int_\Omega Y dP$.

$~$

$~$

A generalização da integral de Lebesgue é feita usando resultados como o *Lema de Fatou* e os teoremas da *convergência monótona* e da *convergência dominada*. Aqui será apresentado apenas uma ideia dessa extensão. Para maiores detalhes, veja as referências citadas anteriormente [@Ash00; @Schervish12; @Billingsley86; @Shiryaev96].

$~$

$~$

**Definição:** Seja $X:\Omega\longrightarrow \mathbb{R}_+$ uma função mensurável não negativa e considere o conjunto de funções $\mathcal{C}_X$ $= \{ f:\Omega\longrightarrow \mathbb{R}_+~,~~f~~\text{simples}~,~~f \leq X\}$. O *valor esperado de $X$* é
$$E[X]=\int_\Omega XdP=\sup\left\{\int_\Omega fdP: f\in \mathcal{C}_X\right\}~.$$

$~$

**Resultado:** Para toda função $X:\Omega \longrightarrow \mathbb{R}_+$, existe uma sequência $(X_n)_{n\geq 1}$ de funções simples não-negativas tais que $X_n(\omega)\leq X_{n+1}(\omega)$, $\forall \omega \in \Omega$, $\forall n \in \mathbb{N}$ com $X_n(\omega)\uparrow X(\omega)$, $\forall \omega \in \Omega$.

$~$

**Exemplo** de sequência $(X_n)_{n\geq 1}$ atendendo as condições anteriores

Para cada $n$, considere $1+n2^n$ conjuntos em $\mathcal{A}:$

+ $E_j^n = \left\{\omega \in \Omega: \dfrac{j}{2^n} \leq X(\omega) \leq \dfrac{j+1}{2^n} \right\}$, $j = 0,1,\ldots,n2^n-1.$

+ $E_{n2^n}^n = \Big\{ \omega \in \Omega: X(\omega)\geq n  \Big\}$ 

e defina $\displaystyle X_n(\omega) = \sum_{j=0}^{n2^n} \dfrac{j}{2^n} ~\mathbb{I}_{E_j^n}(\omega)$. Pode-se provar que $(X_n)_{n\geq 1}$ é tal que  

+ $X_n$ é simples, $\forall n \geq 1$  

+ $X_n \leq X_{n+1}$  

+ $X_{n}(\omega) \uparrow X(\omega)$

$~$

A primeira função dessa sequência é

$X_1(\omega)$ 
$= \displaystyle\sum_{i=0}^2 \frac{i}{2}~\mathbb{I}_{{E}_i^1}(\omega)$ 
$=\displaystyle\left\{\begin{array}{ll}0,&\omega\in{E}_0^1\\
0.5,&\omega\in{E}_1^1\\1,&\omega\in{E}_2^1 \end{array}\right.$.

```{r X1w, echo=FALSE}
# f=w*beta(a1,b1)+(1-w)*beta(a2,b2)
a1=5; b1=12
a2=10; b2=3 
k=5; x=2
w=0.5
theta = seq(0,1,0.001)
A =  w*choose(k,x)*gamma(a1+b1)/(gamma(a1)*gamma(b1))*
  (gamma(a1+x)*gamma(b1+k-x))/gamma(a1+b1+k)
B = (1-w)*choose(k,x)*gamma(a2+b2)/(gamma(a2)*gamma(b2))*
  (gamma(a2+x)*gamma(b2+k-x))/gamma(a2+b2+k)
w2 = A/(A+B)
dpost = w2*dbeta(theta,a1+x,b1+k-x)+
  (1-w2)*dbeta(theta,a2+x,b2+k-x)
En=c(0,0.137,0.166,0.481,0.801,1)
xbreak=c(0,0.0685,0.137,0.1515,0.166,0.3235,0.481,0.641,0.801,0.9005,1)
xlab=c(0,expression(E[0]),"",expression(E[1]),"",expression(E[2]),0.481,expression(E[1]),0.801,expression(E[0]),1)
xcol=c("black","black","black","darkgreen","black","darkred","black","darkgreen","black","black","black")
fn=c(0,0.5,1,0.5,0)
ggplot() + theme_bw() +
  geom_line(data=tibble(w=theta,X=dpost),aes(x=w,y=X,colour="0"),size=1.1) +
  geom_segment(aes(x=En[-length(En)],xend=En[-1],y=fn,yend=fn),size=1.5) +
  geom_segment(aes(x=En[-length(En)],xend=En[-1],y=0,yend=0,colour=c("1","2","3","2","1")),size=1.5) +
  scale_color_manual(values=c("#E76BF3","black","green", "red")) +
  theme(legend.position = "none") +
  theme(axis.text.x=element_text(color=xcol)) +
  scale_x_continuous(breaks=xbreak,labels=xlab) +
  scale_y_continuous(breaks=c(0,0.5,1,2,3)) +
  xlab(expression(w)) + ylab(expression(X(w))) +
  theme(panel.grid.minor.x=element_blank())
```


O gráfico a seguir mostra os quatro primeiras funções da sequência $\left(X_n\right)_{n\geq 1}$ e é possível ter uma ideia da convergência para $X$.

```{r convergXn, echo=FALSE}
# f=w*beta(a1,b1)+(1-w)*beta(a2,b2)
a1=5; b1=12
a2=10; b2=3 
k=5; x=2
w=0.5
theta = seq(0,1,0.001)
A =  w*choose(k,x)*gamma(a1+b1)/(gamma(a1)*gamma(b1))*
  (gamma(a1+x)*gamma(b1+k-x))/gamma(a1+b1+k)
B = (1-w)*choose(k,x)*gamma(a2+b2)/(gamma(a2)*gamma(b2))*
  (gamma(a2+x)*gamma(b2+k-x))/gamma(a2+b2+k)
w2 = A/(A+B)
dpost = w2*dbeta(theta,a1+x,b1+k-x)+
  (1-w2)*dbeta(theta,a2+x,b2+k-x)
#Criando as funções "simples"
n=4
w<-as.vector(apply(matrix(seq(1,n)),1,
  function(x){seq(0/(2^x),(x*(2^x))/(2^x),1/(2^x))}))        
En<-list()         
for(i in 1:length(w)){
  En[[i]]<-rep(0,length(dpost))
}
for(l in 1:(length(En))){
  for(j in 1:(length(w[[l]])-1)){ 
    for(i in 1:length(dpost)){
      if(dpost[i]<w[[l]][j+1]&dpost[i]>=w[[l]][j]){
        En[[l]][i]<-w[[l]][j]
      }else{
        En[[l]][i]<-En[[l]][i]
      }
    }
  }          
}
for(l in 1:(length(En))){
  for(j in length(w[[l]])){ 
    for(i in 1:length(dpost)){
      if(dpost[i]>=w[[l]][j]){
        En[[l]][i]<-w[[l]][j]
      }else{
        En[[l]][i]<-En[[l]][i]
      }
    }
  }          
}
# Criando o gráfico 
if(knitr::is_latex_output()){
tibble(x=as.factor(rep(seq(1,n),each=length(theta))),
       En = unlist(En), theta = rep(theta,n), post = rep(dpost,n)) %>% 
  ggplot() + 
  geom_step(aes(x=theta,y=En, colour=x),lwd=1) +
  geom_line(aes(x=theta,y=post, colour="f"),lwd=1)+
  xlab(expression(w)) + ylab(expression(f[n](w))) +
  theme_bw()+
  scale_color_discrete(name="n")+
  facet_wrap(~x)
} else {
tibble(x=as.factor(rep(seq(1,n),each=length(theta))),
       En = unlist(En), theta = rep(theta,n), post = rep(dpost,n)) %>% 
  ggplot() + 
  geom_step(aes(x=theta,y=En, colour=x),lwd=1) +
  geom_line(aes(x=theta,y=post, colour="X"),lwd=1)+
  xlab(expression(w)) + ylab(expression(X[n](w))) +
  theme_bw()+
  scale_color_discrete(name="n")+
  gganimate::transition_states(x)
}
```


$~$

$~$

**Resultado:** $X,Y: \Omega \longrightarrow\mathbb{R}_+,$ com $X\leq Y$. Então $E[X] \leq E[Y]$.

> **Demo:** Como $X \leq Y$ (isto é, $X(\omega) \leq Y(\omega)$ $\forall \omega \in \Omega$), $\mathcal{C}_X \subseteq \mathcal{C}_Y$  
$\Rightarrow \sup\left\{\displaystyle\int_\Omega f~dP:~ f\in \mathcal{C}_X\right\} \leq \sup\left\{\displaystyle\int_\Omega g~dP:~ g\in \mathcal{C}_Y\right\}$ $\Rightarrow \displaystyle\int_\Omega XdP \leq \displaystyle\int_\Omega YdP$.  

$~$

**Definição**: Seja $X:\Omega \longrightarrow\mathbb{R}_+$ e $E \in \mathcal{A}$ definimos $E(X~\mathbb{I}_E) = \displaystyle\int_EXdP$ $=\displaystyle\int_\Omega X~\mathbb{I}_EdP$.  
Se $E,F \in \mathcal{A}$ com $E\subseteq F$, $\displaystyle\int_E XdP \leq \int_F XdP.$  

$~$

$~$

**Propriedades:** se $X, Y: \Omega \longrightarrow \mathbb{R}_+$ são funções mensuráveis positivas, então  

**1.** $\displaystyle\int_\Omega cXdP  =$ $c\displaystyle\int_\Omega XdP, c\geq 0$;  

**2.** $\displaystyle\int_\Omega (X+Y)dP =$ $\displaystyle\int_\Omega XdP + \int_\Omega YdP$.  


> **Demo 1.** Seja $X_n\uparrow X,$ $X_n \geq 0$ simples. Então $cX_n\uparrow cX,$ $cX_n \geq 0,$ simples.  
$\displaystyle\int_\Omega cX dP$ $=\displaystyle\lim_{n\rightarrow\infty}\int_\Omega cX_n dP$ $=\displaystyle\lim_{n\rightarrow\infty}c\int_\Omega X_n dP$ $=\displaystyle c\lim_{n\rightarrow\infty}\int_\Omega X_n dP$ $=\displaystyle c\int_\Omega X dP$.

> **Demo 2.** Exercício.

$~$

> **Exemplo:** Suponha que $X$ assume valores em $\mathbb{N}$. Pode-se escrever $X =\displaystyle\sum_{i=1}^\infty i ~\mathbb{I}_{A_i}~$, com $A_i = X^{-1}\left(\{i\}\right)$.  
Defina $X_n =\displaystyle\sum_{i=1}^{n-1} i ~\mathbb{I}_{A_i}+n~\mathbb{I}_{\underset{j=n}{\cup} A_j}$. Então $X_n$ é simples, $X_n \geq 0~$, $X_n \leq X_{n+1}$ e $X_n \uparrow X$, de modo que $E(X)$ $=\displaystyle\int_\Omega X dP$ $=\displaystyle\lim_{n \rightarrow\infty}\int_\Omega X_n dP$. Além disso,  
$\displaystyle\int_\Omega X_n dP$ $=\displaystyle\sum_{i=1}^{n-1} i~P(A_i) + n~P\left(\bigcup_{j=n}^{\infty} A_j\right)$ $=\displaystyle\sum_{i=1}^{n-1}i~P(X = i) + n~P(X \geq n)$ $=\displaystyle\sum_{i=1}^{n-1} \sum_{j=1}^{i} P(X = i) + n~P(X \geq n)$ $\displaystyle=\sum_{j=1}^{n-1} \sum_{i=j}^{n-1} P(X = i) + n~P(X \geq n)$ $=\displaystyle\sum_{j=1}^{n-1}P(j \leq X \leq n-1) + n~P(X \geq n)$ $=\displaystyle\sum_{j=1}^n P(X \geq j)$,  
então, $E(X)$ $\displaystyle=\lim_{n\rightarrow \infty}\sum_{j=1}^nP(X \geq j)$ $\displaystyle=\sum_{j=1}^{\infty}P(X \geq j)$.

$~$

Seja $X: \Omega \longrightarrow \mathbb{R}$ e $X^-,X^+: \Omega \longrightarrow \mathbb{R}$ dados por

+ $X^- = \max\{-X,0\}~$ (parte negativa de $X$) e 

+ $X^+ = \max\{X,0\}~$ (parte positiva de $X$)

$~$

```{r Xmaismenos, echo=FALSE}
w=seq(-1.5,2.2,0.01)
tibble(w) %>% mutate(X=w^3-w^2-2*w+1,
               'X-'=ifelse(X<=0,-X,0),'X+'=ifelse(X>=0,X,0)) %>% 
  gather("função","X",-w) %>% 
  ggplot() + theme_bw() + 
  geom_line(aes(x=w,y=X,colour=função,linetype=função),size=1.2) +
  labs(colour="",linetype="")
```

$~$

Note que $X = X^+ - X^-$

$~$

Se $\displaystyle\int_\Omega X^+ dP < \infty$ ou $\displaystyle\int_\Omega X^- dP < \infty$, definimos 

$E[X]$ $=\displaystyle\int X dP$ $=\displaystyle\int_\Omega X^+dP - \int_\Omega X^- dP$ $=E\left[X^+\right] - E\left[X^-\right]$.

$~$

Além disso, seja $|X| = X^+ + X^-$. Então, $E\left[~|X|~\right] < \infty$ se $E(X^+) < \infty$ e $E(X^-) < \infty$, e, nesse caso, dizemos que $X$ é *integrável*.

$~$

**Propriedades:** se $X, Y: \Omega \longrightarrow \mathbb{R}$ são funções mensuráveis, então 

**1.** $X \leq Y \Rightarrow E(X) \leq E(Y)$;  

**2.** $c \in \mathbb{R},$ $E(cX) = cE(X)$;  

**3.** $X,Y$ integráveis. $E(X+Y) = E(X) + E(Y)$.

> **Demo 1.**  
$X \leq Y \Rightarrow$ $\left\{\begin{array}{c}X^+ \leq Y^+\\
X^- \geq Y^-\end{array}\right.$  
$E(X) =$ $E(X^+) - E(X^-)$ $\leq E(Y^+) - E(Y^-)$ $=E(Y).$

> **Demo 2.**  
$(cX)^+ =$ $\left\{\begin{array}{rcl}cX^+ &,& c \geq 0\\
-cX^- &,& c < 0 \end{array}\right.$  
$(cX)^- =$ $\left\{\begin{array}{rcl}cX^- &,& c \geq 0\\
-cX^+ &,& c < 0 \end{array}\right.$  
Para $c<0$,  
$E[cX]$ $= E[(cX)^+] - E[(cX)^-]$ $= E[-cX^-] - E[-cX^+]$ $= -cE[X^-] + cE[X^+]$ $= cE[X]$.

> **Demo 3.**   
$\displaystyle\int_\Omega \left(X^+ + Y^+\right) dP < \infty$ ou $\displaystyle\int_\Omega \left(X^- + Y^-\right) dP < \infty$  
$X + Y$ $= (X + Y)^+ - (X+Y)^-$ $= X^+ - X^- + Y^+ - Y^-$  
$\Rightarrow (X+Y)^+ + X^- + Y^-$ $= X^+ + Y^+ + (X+Y)^-$  
$\Rightarrow \displaystyle\int_\Omega (X+Y)^+dP + \int_\Omega X^-dP + \int_\Omega Y^-dP$  
$=\displaystyle\int_\Omega X^+dP + \int_\Omega Y^+dP + \int_\Omega (X+Y)^-dP$.  
$|X+Y|$ $= |X^+-X^-+Y^+-Y^-|$ $\leq X^++X^-+Y^++Y^-$  
$\Rightarrow \displaystyle\int_\Omega (X+Y)^+dP - \int_\Omega(X+Y)^-dP$ $=\displaystyle \int_\Omega X^+dP -\int_\Omega X^-dP + \int_\Omega Y^+dP -\int_\Omega Y^-dP$.  
$\Rightarrow \displaystyle\int_\Omega(X+Y)dP = \int_\Omega XdP + \int_\Omega YdP$

$~$

$~$

## Funções de Variáveis Aleatórias

Considere agora uma v.a. $X: \Omega \longrightarrow \mathbb{R}$ e uma função real $g: \mathbb{R} \longrightarrow \mathbb{R}$. Defina $Y = g(X)$. Então

$$(\Omega, \mathcal{A},P) \overset{X}{\longrightarrow}(\mathbb{R},\mathcal{B}(\mathbb{R}),P_X)\overset{g}{\longrightarrow}(\mathbb{R},\mathcal{B}(\mathbb{R}),P_Y)$$

$$(\Omega, \mathcal{A},P)\overset{Y = g(X)}{\longrightarrow}(\mathbb{R},\mathcal{B}(\mathbb{R}),P_Y)$$

Logo, se $g$ é uma função mensurável, $Y=g(X)$ também é v.a. e as medidas induzidas por X e Y são

$P_X(A)$ $= P(X^{-1}(A))$ $= P\left(\{\omega \in \Omega : X(\omega) \in A\}\right)$;

$P_Y(B)$ $= P_X(g^{-1}(B))$ $= P_X\left(\{x \in \mathbb{R} : g(x) \in B\}\right)$ $= P\left(\{\omega \in \Omega : g\left(X(\omega)\right) \in B\}\right)$.

Assim, uma pergunta natural é como obter o valor esperado de $Y$.

$E(Y) = \displaystyle\int_\Omega YdP$ $=\displaystyle\int_\Omega g(X)dP$ $\overset{?}{=} \displaystyle\int_{\mathbb{R}}g~dP_X$. 

$~$

> **Caso 1.** Seja $g:\mathbb{R}\longrightarrow\mathbb{R}_+$ uma função simples tal que $g = \sum_{i=1}^kg_i~\mathbb{I}_{B_i}$, $g_1,\ldots,g_k \in \mathbb{R}$ e $B_1,\ldots,B_k \in \mathcal{B}(\mathbb{R})$. Então,  
$\displaystyle\int_\Omega Y~dP$ 
$=\displaystyle\int_\Omega g(X)~dP$ 
$=\displaystyle\int_\Omega \left(\sum_{i=1}^k g_i ~\mathbb{I}_{B_i}(X)\right)dP$ 
$=\displaystyle\int_\Omega \left(\sum_{i=1}^k g_i ~\mathbb{I}_{X^{-1}(B_i)}\right)dP$ 
$~\displaystyle\overset{def}{=}~\sum_{i=1}^k g_i~P(X^{-1}(B_i))$ 
$=\displaystyle\sum_{i=1}^k g_i~P_X(B_i)$ 
$=\displaystyle\int_{\mathbb{R}}\left(\sum_{i=1}^kg_i~\mathbb{I}_{B_i}\right)dP_X$ 
$=\displaystyle\int_{\mathbb{R}} g~dP_X$.

$~$

> **Caso 2.** Seja $g:\mathbb{R}\longrightarrow\mathbb{R}_+$ uma função não negativa e $(g_n)_{n\geq1}$, $g_n \geq 0$, uma sequência crescente de funções simples tal que $g_n\uparrow g$. Como $g_n$ é simples,  
$\displaystyle\int_\Omega g_n(X)dP$ $=\displaystyle\int_{\mathbb{R}}g_n~dP_X$ $\displaystyle~\underset{n\uparrow\infty}{\longrightarrow}~ \int_\Omega g(X)dP$ $=\displaystyle\int_{\mathbb{R}}g~dP_X$.

$~$

> **Caso 3.** Agora para $g: \mathbb{R} \longrightarrow \mathbb{R}$, temos  
$\displaystyle\int_\Omega g^+(X)dP$ $=\displaystyle\int_{\mathbb{R}}g^+dP_X$ e $\displaystyle\int_\Omega g^-(X)dP$ $=\displaystyle\int_{\mathbb{R}}g^-dP_X$.  
Logo, $\displaystyle\int_\Omega g(X)dP$ $=\displaystyle\int_{\mathbb{R}}g~dP_X$.

$~$

$~$

Suponha agora $X$ v.a. discreta assumindo valores em $\{x_1,x_2,\ldots\}$ com probabilidade $1$. Nesse caso, para $A\subseteq\mathcal{B}(\mathbb{R})$,

$P_X(A)$ $=P_X(X \in A)$ $=P\left(\{\omega\in\Omega:  X(\omega) \in A\}\right)$ $=\displaystyle\sum_{i:~x_i\in A} P_X(X=x_i)$. 

Vamos "verificar" que $E\left[g(X)\right]$ $=\displaystyle\sum_{i=1}^\infty g(x_i)P_X(X=x_i)$.

> **Caso 1.** $g$ simples com $g = \displaystyle\sum_{i=1}^kg_i~\mathbb{I}_{B_i}$, $g_1,\ldots,g_k \in \mathbb{R}_+$ $B_1,\ldots,B_k \in \mathcal{B}(\mathbb{R})$. Então,  
$E\left[g(X)\right]$ $=\displaystyle\int_\Omega g(X)~dP$ 
$=\displaystyle\sum_{i=1}^k g_i~P\left(X^{-1}(B_i)\right)$ 
$=\displaystyle\sum_{i=1}^k g_i~P_X(B_i)$ 
$=\displaystyle\sum_{i=1}^k g_i \sum_{j:~x_j \in B_i}^k P_X(X = x_j)$ 
$=\displaystyle\sum_{i=1}^k g_i \sum_{j=1}^\infty \mathbb{I}_{B_i}(x_j)P_X(X=x_j)$ 
$=\displaystyle\sum_{j=1}^\infty \underbrace{\left(\sum_{i=1}^k g_i ~\mathbb{I}_{B_i}(x_j)\right)}_{g(x_j)}P_X(X = x_j)$.

$~$ 

> **Caso 2.** $g\geq 0,$ $g_n\geq0,$ $g_n$ simples tal que $g_n \uparrow g$  
$\displaystyle\int_\Omega g(X)dP$ $=\displaystyle\lim_{n\rightarrow\infty}\int_\Omega g_n(X)dP$ $=\displaystyle\lim_{n\rightarrow\infty}\left\{\sum_{j=1}^\infty g_n(x_j)P_X(X=x_j)\right\}$ $=\displaystyle\sum_{j=1}^\infty g(x_j)P_X(X = x_j)$

$~$

> **Caso 3.** Agora para $g: \mathbb{R} \longrightarrow \mathbb{R}$, temos  
$\displaystyle\int_\Omega g^+(X)dP$ 
$=\displaystyle\sum_{j=1}^\infty g^+(x_j)P_X(X = x_j)$ e 
$\displaystyle\int_\Omega g^-(X)dP$ 
$=\displaystyle\sum_{j=1}^\infty g^-(x_j)P_X(X = x_j)$.  
Logo, $\displaystyle\int_\Omega g(X)dP$ $=\displaystyle\sum_{j=1}^\infty g(x_j)P_X(X = x_j)$.

$~$

$~$


Suponha agora $X$ v.a. absolutamente contínua com função de densidade de probabilidade $f_X$, ou seja, pode-se escrever $P_X(X\in A)$ $=\displaystyle\int_Af_X(t)dt$ $=\displaystyle\int_{\mathbb{R}}\mathbb{I}_A(t)f_X(t)dt$. Vamos "verificar" que $E\left[g(X)\right]$ $=\displaystyle\int_{\mathbb{R}} g(x)f_X(x)dx$.

> **Caso 1.** $g$ simples com $g = \displaystyle\sum_{i=1}^kg_i~\mathbb{I}_{B_i}$, $g_1,\ldots,g_k \in \mathbb{R}_+$ $B_1,\ldots,B_k \in \mathcal{B}(\mathbb{R})$. Então,  
$E\left[g(X)\right]$ $=\displaystyle\int_\Omega g(X)~dP$ 
$=\displaystyle\int_\Omega\left(\sum_{i=1}^k g_i~\mathbb{I}_{B_i}(X)\right)dP$
$=\displaystyle\int_\Omega\left(\sum_{i=1}^k g_i~\mathbb{I}_{X^{-1}(B_i)}\right)dP$ 
$=\displaystyle\sum_{i=1}^k g_i~P(X^{-1}(B_i))$ 
$=\displaystyle\sum_{i=1}^k g_i~P_X(B_i)$ 
$=\displaystyle\sum_{i=1}^k g_i~\int_{\mathbb{R}}\mathbb{I}_{B_i}(x)f_X(x)dx$ 
$=\displaystyle\int_{\mathbb{R}}\sum_{i=1}^k g_i\mathbb{I}_{B_i}(x)f_X(x)dx$
$=\displaystyle\int_{\mathbb{R}}g(x)f_X(x)dx$.

> A extensão para funções positivas e para funções reais é análogo ao que foi feito nos exemplos anteriores.

$~$ 

$~$

Assim, em geral, vale que:

* $X$ discreto: $E[g(X)]$ $=\displaystyle\sum_{j=1}^\infty g(x_j)P_X(X=x_j)$;

* $X$ (absolutamente) contínuo: $E[g(X)]$ $=\displaystyle\int_{\mathbb{R}}g(x)f_X(x)dx$.

$~$

Esses resultados valem também se $X: \Omega \longrightarrow \mathbb{R}^k$ e $g: \mathbb{R}^k\longrightarrow \mathbb{R}.$

$~$

$~$

> **Exemplo 1.** Seja $X$ uma v.a. definida em $\mathbb{N}$ com função de probabilidade $P_X(X=x)=\dfrac{e^{-\lambda}\lambda^x}{x!}~\mathbb{I}_{\mathbb{N}}(x)$, para $\lambda>0$ fixado. Dizemos nesse caso que $X \sim \text{Poisson}(\lambda)$. Então, o valor esperado de $X$ é  
$E\left[X\right]$ $=\displaystyle\sum_{x=0}^\infty x~P_X(X=x)$ 
$=\displaystyle\sum_{x=0}^\infty x~\dfrac{e^{-\lambda}\lambda^x}{x!}$ 
$=\displaystyle\sum_{x=1}^\infty \dfrac{e^{-\lambda}\lambda^x}{(x-1)!}$ 
$=\displaystyle\lambda~\sum_{x=1}^\infty \dfrac{e^{-\lambda}\lambda^{x-1}}{(x-1)!}$ 
$~\overset{t=x-1}{=}~\displaystyle\lambda~\sum_{t=0}^\infty \dfrac{e^{-\lambda}\lambda^{t}}{t!}$
$\Longrightarrow E\left[X\right] = \lambda$.  
$~$  
Ainda neste exemplo, considere $g:\mathbb{R}\rightarrow\mathbb{R}$ com $g(t) = e^t$. Então,  
$E\left[g(X)\right]$ $=\displaystyle\sum_{x=0}^\infty g(x)P_X(X=x)$ $=\displaystyle\sum_{x=0}^\infty e^x~\dfrac{e^{-\lambda}\lambda^x}{x!}$ $=\displaystyle e^{-\lambda}\sum_{x=0}^\infty \dfrac{(\lambda e)^x}{x!}$ $=\displaystyle e^{-\lambda}e^{\lambda e}\underbrace{\sum_{x=0}^{\infty} \dfrac{e^{-\lambda e}(\lambda e)^x}{x!}}_{1}$ 
$=e^{\lambda e-\lambda}$ $=e^{\lambda(e-1)}$.

$~$

> **Exemplo 2.** Seja $X$ uma v.a. definida em $[0,1]$ com função densidade de probabilidade $f_X(x)=\dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}~x^{a-1}(1-x)^{b-1}~\mathbb{I}_{[0,1]}(x)$, para $a,b>0$ fixados. Dizemos nesse caso que $X \sim \text{Beta}(a,b)$. Então, o valor esperado de $X$ é  
$E[X]$ $=\displaystyle\int_{-\infty}^\infty x~f_X(x)dx$ 
$=\displaystyle\int_0^1 x~\dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}~x^{a-1}(1-x)^{b-1}dx$ 
$=\displaystyle\dfrac{\Gamma (a+1)}{\Gamma(a+1+b)}\dfrac{\Gamma(a+b)}{\Gamma(a)}\int_0^1\dfrac{\Gamma(a+1+b)}{\Gamma(a+1)\Gamma(b)}~x^{(a+1)-1}(1-x)^{b-1}dx$ 
$=\dfrac{\Gamma (a+1)}{\Gamma(a+1+b)}\dfrac{\Gamma(a+b)}{\Gamma(a)}$.  
$~$  
Considere agora $g:\mathbb{R}\rightarrow\mathbb{R}$ com $g(t) = t^c(1-t)^d$, com $c,d>0$ fixados. Então,  
$E[g(X)]$ $=\displaystyle\int_{-\infty}^\infty g(x)~f_X(x)dx$ 
$=\displaystyle\int_0^1 \dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}~x^{a+c-1}(1-x)^{b+d-1}dx$ 
$=\displaystyle\dfrac{\Gamma (a+c)\Gamma(b+d)}{\Gamma(a+c+b+d)}\dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1\dfrac{\Gamma(a+c+b+d)}{\Gamma(a+b)\Gamma(b+d)}~x^{(a+c)-1}(1-x)^{(b+d)-1}dx$ 
$=\dfrac{\Gamma(a+c)\Gamma(b+d)}{\Gamma(a+c+b+d)}\dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}$
$=\dfrac{\beta(a+c,b+d)}{\beta(a,b)}$.

<!-- 4. $X \sim Geo(\theta)$  -->

<!-- $P(X=x)=$ $(1-\theta)^{x-1}\theta ~\mathbb{I}_{\{1,2\ldots\}}(x)$ como $X$ é inteira não-negativa, vale que: -->

<!-- $E(X)=$ $\sum_{i=1}^\infty P(X \geq 1)=$ $\sum_{i=1}^\infty \left\{\sum_{j=1}^\infty P(X=j)\right\}=$ $\sum_{i=1}^\infty \left\{\sum_{j=1}^\infty (1-\theta)^{j-1}\theta\right\}=$ $\sum_{i=1}^\infty (1-\theta)^{i-1}\Rightarrow$ $E(x)=\dfrac{1}{\theta}$ -->

<!-- Se $X$ é contínua não-negativa, então -->

<!-- $E(X)=$ $\int_0^\infty P(X>t)dt.$ -->

<!-- **Exemplo** -->

<!-- $X\sim Exp(\lambda)$ -->

<!-- $f_X(x)=\lambda e^{\lambda x}~\mathbb{I}_{\mathbb{R}_+}(x)$ -->

<!-- $P(X > t)=$ $\int_t^\infty \lambda e^{-\lambda s}ds=$ $e^{-\lambda t}$ -->

<!-- Assim, $E(X)=$ $\int_0^\infty P(X>t)dt=$ $\dfrac{1}{\lambda}\underbrace{\int_0^\infty \lambda e^{-\lambda t}dt}_{1} \Rightarrow$ $E(X)=\dfrac{1}{\lambda}$ -->

<!-- 5. $(X,Y)$ absolutamente contínuo com densidade -->

<!-- $f(x,y)=$ $\dfrac{1}{y}e^{-y}~\mathbb{I}_{(0,y)}(x)~\mathbb{I}_{\mathbb{R}_+}(y);$ $g(x,y)=xy$  -->

<!-- $E(g(X,Y))=?$ -->

<!-- $E(XY)=$ $\int_{-\infty}^\infty \int_{-\infty}^\infty xyf(x,y)dxdy=$ $\int_{-\infty}^\infty \left[\int_{-\infty}^\infty xy\dfrac{1}{y}e^{-y}dx\right]dy=$ $\int_{0}^\infty \dfrac{y^2}{2}e^{-y}dy=$ $\dfrac{1}{2}\dfrac{\Gamma(3)}{1^3}\int_0^\infty \dfrac{1^3}{\Gamma(3)}y^2e^{-y}dy \Rightarrow$ $E(XY)=1$ -->

<!-- 6. $(X_1,\ldots,X_k) \sim DIR(a_1,\ldots,a_k)$ -->

<!-- $g(X_1,\ldots,X_k) = X_1^{n_1}X_2^{n_2}\cdots X_k^{n_k}(1-X_1-\ldots-X_k)^{n_0}$ -->

<!-- $E[g(X_1,\ldots,X_k)]=$ $\int_{S_k} X_1^{n_1}\cdots X_k^{n_k}(1-X_1-\ldots-X_k)^{n_0}$ $\underbrace{\dfrac{\Gamma(a_0+a_1+\ldots+a_k)}{\Gamma(a_0)\Gamma(a_1)\ldots\Gamma(a_k)}}_{c(a_0,a_1,\ldots,a_k)}$ $x_1^{a_1-1}x_2^{a_2-1}\ldots x_k^{a_k-1}$ $(1-x_1-\ldots-x_k)^{a_0-1}dx,$ -->

<!-- onde $S_k = \left\{(y_1,y_2,\ldots,y_k)\in \mathbb{R}^K_+: y_1+\ldots+y_k \leq 1\right\}$ -->

<!-- Então, $E[g(y_1,\ldots,y_k)]=$ $c(a_0,a_1,\ldots,a_k)$ $\int_{S_K}x_1^{a_1+n_1-1}\ldots x_k^{a_k+n_k-1}$ $(1-x_1-\ldots-x_k)^{a_0+n_0-1}dx$ -->

<!-- $\Rightarrow E[g(x_1,\ldots,x_k)]=$ $\dfrac{c(a_0,\ldots,a_k)}{c(a_0+n_0,a_1+n_1,\ldots,a_k+n_k)}$ -->

<!-- 7. n lançamentos de uma moeda. Dizemos que ocorre um "rum" de tamanho $k$ se são observadas $k$ caras consecutivas. -->

<!-- $X:$ Número de lançamentos de "run" de tamanho $k$ observados. -->

<!-- $n=4$ $cc\bar{c}c$ -->

<!-- $k=2$ $ccc\bar{c}$ -->

<!-- Definimos  -->

<!-- $X_i=\left\{\begin{array}{ll} -->
<!-- 1, & \text{ se ocorre rum de tamanho k iniciando no i=ésimo lançamento}\\ -->
<!-- 0 & c.c. -->
<!-- \end{array}\right.$ -->

<!-- $X=\sum_{i=1}^{n-k+1}X_i$ -->

<!-- $E(X) = E\left(\sum_{i=1}^{n-k+1}X_i\right)=$ $=\sum_{i=1}^{n-k+1}E(X_i)=$ $\sum_{i=1}^{n-k+1}\left\{1P(X_i=1)+0P(X_i=0)\right\}=$ $\sum_{i=1}^{n-k+1}P(X_i=1)=$ $\sum_{i=1}^{n-k+1}p^k \Rightarrow$  -->

<!-- $E(X)=(n-k+1)p^k.$ -->

<!-- 8. Problema dos pareaentos ($n$ objetos) -->

<!-- $X:$ NÚmero de pareamentos -->

<!-- $X = X_1+X_2+\ldots+X_n$ onde -->

<!-- $X_i=\left\{\begin{array}{ll} -->
<!-- 1, & \text{há areamento na i-ésima posição}\\ -->
<!-- 0, & c.c.\end{array}\right.$ -->

<!-- $E(X)=$ $E\left(\sum_{i=1}^n X_i\right)=$  $\sum_{i=1}^n E(X_i)=$ $\sum_{i=1}^nP(X_i=1)=$ $\sum_{i=1}^n \dfrac{(n-1)!}{n!} \Rightarrow$ $E(X)=1$ -->

$~$

$~$

## Função de Distribuição

**Definição:** Uma função $F: \mathbb{R} \longrightarrow [0,1]$ é uma *função de distribuição* (f.d.) se

(i) $F$ é não-decrescente e contínua à direita;
(ii) $\displaystyle\lim_{x\downarrow-\infty}F(x)=0$ e $\displaystyle\lim_{x\uparrow+\infty}F(x)=1$.

$~$

**Proposição:** Se $X$ é uma v.a., então $F_X(x)=P_X(X\leq x)$ é uma f.d. Recíprocamente, se $F_X$ é uma f.d, então existe uma v.a. $X$ com f.d. $F_X$.

$~$

* É possível usar uma f.d. $F$ para criar uma medida em $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. Para tal, defina $P\left((a,b]\right)=F(b)-F(a)$ e essa medida pode ser estendida para a $\sigma$-álgebra usando o Teorema de Extensão de Caratheodory (veja, por exemplo, @Schervish12, pág. 578).

* Reciprocamente, se $P$ é uma medida de probabilidade em $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ então $F(x)=P\left((-\infty,x]\right)$ é uma f.d. 

* Neste caso, se $g: \mathbb{R}\longrightarrow \mathbb{R}$ é uma função mensurável, não será feita distinção entre $\displaystyle\int g(x)dF(x)=$ $\displaystyle\int g(x)~dP_X(x)$.

* Se $P$ é uma medida de probabilidade em $(\mathbb{R}^k,\mathcal{B}(\mathbb{R}^k))$ então $F(x_1,\ldots,x_k)=$ $P((-\infty,x_1]\times \ldots\times (-\infty,x_k])$ é a *função de distribuição conjunta* do *vector aleatório* $\boldsymbol{X} = (X_1,\ldots,X_K)$.

$~$

**Definição:** Uma função de distribuição é dita 

(i) *Discreta* se existe um conjunto enumerável $B=\{x_1,x_2,\ldots\}\subset \mathbb{R}$ tal que $P_X(B)=1$ e $F_d(x)=\displaystyle\sum_{x_i\leq x} P_X(X=x_i)$. Nesse caso, $f(x_i)=P_X(X=x_i)$ é chamada *função de probabilidade* de $X$; 

(ii) *Absolutamente Contínua* é contínua se existe $f: \mathbb{R} \rightarrow \mathbb{R}$ tal que $P_X\left((a,b]\right)=F_c(b)-F_c(a) = \displaystyle\int_{a}^{b} f(t)~dt$. A função $f$ é a *função de densidade de probabilidade* de $X$;

(iii) *Singular* se $F_s$ é contínua com $F_s'=0~$ $[\lambda]$ q.c. ($F_s$ é singular com relação à medida de Lebesgue $\lambda$).

**Resultado:** Toda f.d. $F$ pode ser escrita como $F=\alpha_1F_d+\alpha_2F_c+(1-\alpha_1+\alpha_2)F_s$, com $\alpha_1,\alpha_2\geq 0$ tal que $\alpha_1+\alpha_2\leq 1$. 

$~$

$~$

**Definição:** Seja $(\Omega,\mathcal{A})$ um espaço mensurável e $\mu_1$ e $\mu_2$ medidas nesse espaço. Dizemos que $\mu_2$ é *absolutamente contínua* com relação à $\mu_1$ se, $\forall A \in \mathcal{A}$, $\mu_1(A)=0$ $~\Rightarrow~ \mu_2(A)=0$.

+ Nesse caso, dizemos que $\mu_2$ é dominada por $\mu_1$ ou que $\mu_1$ é uma medida dominante para $\mu_2$ e denotamos $\mu_2 \ll \mu_1$.

$~$

**Teorema (de Radon-Nikodin):** Seja $\mu_2 \ll \mu_1$ com $\mu_1$ $\sigma$-finita. Então, $\exists f: \Omega \longrightarrow [0,+\infty]$ tal que, $\forall A \in \mathcal{A}$,
$$\mu_2(A) = \int_A f(x) d\mu_1(x).$$
Além disso, se $g:\Omega \longrightarrow \mathbb{R}$ é $\mu_2$-integrável, então
$$\int g(x) d\mu_2(x) = \int g(x) f(x) d\mu_1(x).$$
A função $f=\frac{d\mu_2}{d\mu_1}$ é chamada de derivada de Radon-Nikodin da medida $\mu_2$ com relação à medida $\mu_1$ e é única $[\mu_1]$ q.c. (ou seja, é única em todo conjunto $\Omega$ com eventual excessão de um conjunto $C$ tal que $\mu_1(C)=0$).


$~$

$~$


**Definição:** $(\Omega, \mathcal{A}, P)$ espaço de probabilidade e $(\mathfrak{X},\mathcal{F},\mu)$ espaço mensurável. Considere $X: \Omega \longrightarrow \mathfrak{X}$ uma v.a. e $P_X$ a medida induzida por $X$ de $P$, i.e. $P_X(B) = P(X^{-1}(B))$. Suponha que $P_X \ll \mu$. Então, a derivada de Radon-Nikodin $f_X = \dfrac{dP_X}{d\mu}$ é chamada *densidade de $X$ com respeito à $\mu$*.

**Proposição:** Se $h: \mathfrak{X}\longrightarrow\mathbb{R}$ é mensurável e $f_X = \dfrac{dP_X}{d\mu}$ é a densidade de $X$ com respeito à $\mu$, então $\displaystyle\int h(x)dF_X(x)$ $=\displaystyle\int h(x)f_X(x)d\mu$.


$~$

> **Exemplo 1:** Seja $\Omega=\mathfrak{X}=\mathbb{R}$ com a $\sigma$-álgebra de Borel e $f$ uma função não negativa tal que $\displaystyle\int f(x) dx = 1$. Defina $\displaystyle P(A)= \int_A f(x) dx$ e $X(\omega)=\omega$. Então, $X$ é uma variável aleatória absolutamente contínua com função de densidade de probabilidade (f.d.p.) $f$ e $P_X = P$. Além disso, $P_X$ é absolutamente contínua com relação à medida de Lebesgue $(P_X \ll \lambda)$ e $\frac{dP_X}{d\lambda}=f$.

```{r FDcontinua, echo=FALSE}
# f=w*beta(a1,b1)+(1-w)*beta(a2,b2)
a1=5; b1=12
a2=10; b2=3 
k=5; x=2
w=0.5
theta = seq(0,1,0.001)
A =  w*choose(k,x)*gamma(a1+b1)/(gamma(a1)*gamma(b1))*
  (gamma(a1+x)*gamma(b1+k-x))/gamma(a1+b1+k)
B = (1-w)*choose(k,x)*gamma(a2+b2)/(gamma(a2)*gamma(b2))*
  (gamma(a2+x)*gamma(b2+k-x))/gamma(a2+b2+k)
w2 = A/(A+B)
dpost = w2*dbeta(theta,a1+x,b1+k-x)+
  (1-w2)*dbeta(theta,a2+x,b2+k-x)
ppost = w2*pbeta(theta,a1+x,b1+k-x)+
  (1-w2)*pbeta(theta,a2+x,b2+k-x)
g1=tibble(x=theta,fx=dpost) %>% 
  ggplot() + theme_bw() +
  geom_line(aes(x=x,y=fx))
g2=tibble(x=theta,Fx=ppost) %>% 
  ggplot() + theme_bw() +
  geom_line(aes(x=x,y=Fx))
gridExtra::grid.arrange(g1,g2)
```

$~$

> **Exemplo 2:** Seja $\Omega=\mathbb{R}$ com a $\sigma$-álgebra de Borel, $\mathfrak{X} = \{x_1,x_2,\ldots\}$ um conjunto enumerável. Seja $f$ uma função não negativa definida em $\mathfrak{X}$ tal que $\displaystyle \sum_{i=1}^{\infty} f(x_i) = 1$. Defina $\displaystyle P_X(A) = \sum_{\{i:~x_i \in A\}} f(x_i)$. Então $X$ é uma variável aleatória discreta com função de probabilidade (f.d.p.) $f$. Além disso, $P_X$ é absolutamente contínua com relação à medida de contagem $(P_X \ll \nu)$ e $\frac{dP_X}{d\nu}=f$.

```{r FDdiscreta, echo=FALSE}
# f=w*BetaBin(k,a1,b1)+(1-w)*BetaBin(k,a2,b2)
a1=15; b1=60
a2=40; b2=20
k=10; w=0.7
theta = seq(0,k)
dpost = w*extraDistr::dbbinom(theta,k,a1,b1)+
  (1-w)*extraDistr::dbbinom(theta,k,a2,b2)
ppost = w*extraDistr::pbbinom(theta,k,a1,b1)+
  (1-w)*extraDistr::pbbinom(theta,k,a2,b2)
g1=tibble(x=theta,fx=dpost) %>% 
  ggplot() + theme_bw() +
  geom_segment(aes(x=x,y=fx,xend=x,yend=0),size=2) +
  scale_x_continuous(breaks=c(0,1,2,3,4,5,6,7,8,9,10))
g2=tibble(x=theta,Fx=ppost) %>% 
  ggplot() + theme_bw() +
  geom_segment(aes(x=x,xend=x+1,y=Fx,yend=Fx),size=1.5) +
  scale_x_continuous(breaks=c(0,1,2,3,4,5,6,7,8,9,10)) + 
  ylim(0,1)
gridExtra::grid.arrange(g1,g2)
```

$~$

$~$

**Resultado** Sejam $(\Omega,\mathcal{A})$ espaço mensurável, $P_1,P_2: \mathcal{A}\longrightarrow [0,1]$ medidas de probabilidade, $X: \Omega \longrightarrow \mathbb{R}$ v.a. e $P=\alpha P_1+(1-\alpha)P_2$ com $0\leq\alpha\leq1$. Então,$\displaystyle\int_\Omega XdP$ $=\displaystyle\alpha \int_\Omega XdP_1 + (1-\alpha)\int_\Omega XdP_2$.

> **Caso 1.** $X$ simples, $X=\displaystyle\sum_{i=1}^kX_i~\mathbb{I}_{A_i}$.  
$\displaystyle\int_\Omega XdP$ $=\displaystyle\sum_{i=1}^kx_i~P(A_i)$ $=\displaystyle\sum_{i=1}^k x_i[\alpha P(A_i)+(1-\alpha)P_2(A_i)]$ $=\displaystyle\alpha \sum_{i=1}^k x_iP(A_i)+(1-\alpha)\sum_{i=1}^k x_iP_2(A_i)$ $=\displaystyle\alpha \int_\Omega XdP_1+(1-\alpha)\int_\Omega XdP_2$.

> **Caso 2.** $X \geq 0$.  
Considere a sequência $\left(X_n\right)_{n\geq 1}$ tal que $X_n \uparrow X$, $X_n \geq 0$ simples. Então,  
$=\displaystyle\int_\Omega XdP$ $=\displaystyle\lim_{n\rightarrow\infty}\int_\Omega X_n dP$ $=\displaystyle\lim_{n\rightarrow\infty}\left\{\alpha\int_\Omega X_ndP_1+(1-\alpha)\int_\Omega X_ndP_2\right\}$ $=\displaystyle\alpha\lim_{n\rightarrow\infty}\int_\Omega X_ndP_1 + (1-\alpha)\lim_{n\rightarrow\infty}\int_\Omega X_n dP_2$ $=\displaystyle\alpha\int XdP_1 + (1-\alpha)\int_\Omega XdP_2$.

> **Caso 3.** $X$ qualquer.  
Basta escrever $X=X^+-X^-$ e repetir o procedimento anterior.

$~$


Seja $P_1$ uma distribuição discreta com $P_1\left(\left\{x_1,x_2,\ldots,\right\}\right)=1$, $P_2$ uma distribuição absolutamente contínua com função densidade de probabilidade $f_x$ e $X:\Omega \longrightarrow \mathbb{R}$ tal que $P_X(X \in A)=$ $\alpha P_1\left(X^{-1}(A)\right)+(1-\alpha)P_2\left(X^{-1}(A)\right)$. Então,

$E(X)$ $=\displaystyle\int_\Omega XdP$ $=\displaystyle\alpha \int_\Omega XdP_1 + (1-\alpha)\int_\Omega XdP_2$ $=\displaystyle\alpha \sum_{i=1}^\infty x_iP_1(X=x_i)+(1-\alpha)\int_{-\infty}^\infty x f_X(x)dx$

> **Exemplo.** Considere uma v.a. $X$ com f.d. dada por  
$F_X(t)=\left\{\begin{array}{ll}
0, & t<0\\
\dfrac{1}{15}+\dfrac{2}{3}t, & 0\leq t < 1\\
1, & t \geq 1\end{array}\right.$

```{r FDmistura, echo=FALSE}
#x=seq(0,1,length.out=1000)
ggplot() + theme_bw() +
  geom_segment(aes(x=0,xend=1,y=1/15,yend=11/15),size=1.2) +
  geom_segment(aes(x=-0.2,xend=0,y=0,yend=0),size=1.2) +
  geom_segment(aes(x=1,xend=1.2,y=1,yend=1),size=1.2) +
  geom_point(aes(x=c(0,1),y=c(1/15,1)),size=2) +
  xlab("x") + ylab(expression(F[X])) +
  scale_x_continuous(breaks=c(0,0.5,1)) +
  scale_y_continuous(breaks=c(0,1/15,11/15,1),labels=c("0","1/15","11/15","1"))
```

> Temos que $P(X=0)=1/15$, $P(X=1)=4/15$ e, assim, $P(0<X<1)=10/15=2/3=1-\alpha$, de modo que  
$\dfrac{1}{15}$ $=P(X=0)$ $=\alpha P_1(X=0)$  $=1/3~P_1(X=0)$ 
$\Rightarrow P_1(X=0)=\dfrac{1}{5} = 1-P_1(X=1)$.  
$E(X)$ $=\displaystyle\alpha\int_\Omega XdP_1+(1-\alpha)\int_\Omega X dP_2$ $=\displaystyle\dfrac{1}{3}\left\{0\cdot\dfrac{1}{5}+1\cdot\dfrac{4}{5}\right\}+\dfrac{2}{3}\int_{0}^{1} x~f_X(x)dx$ $=\displaystyle\dfrac{1}{3}\cdot\dfrac{4}{5}+\dfrac{2}{3}\int_0^1 xdx$ $=\dfrac{4}{15}+\dfrac{1}{3}$ $=\dfrac{4}{15}+\dfrac{5}{15}$ $=\dfrac{9}{15}$.

$~$

$~$

## Probabilidade Condicional

**Motivação:** $P(B|A)=$ $\dfrac{P(A\cap B)}{P(A)}$ é bem definido se $P(A)>0.$  

Seja $X,Y: \Omega \longrightarrow \mathbb{R}$ v.a. tais que $P_X\left([0,1]\right)=1$ e $P_Y\left(\{0,1\}\right)=1$. Considere um experimento em dois estagios onde seleciona-se $X$ segundo uma distribuição absolutamente contínua $F_X$ e, dado $X=x$, $0\leq x\leq 1$, uma moeda com probabilidade $x$ é lançada $n$ vezes. Nesse caso, é natural definir $Y~\big|~X=x\sim \text{Bin}(n,x)$, mesmo que $P(X=x)=0$, $\forall x \in [0,1]$.

$~$

**Teorema da Medida Produto (para medidas de probabilidade)**  
Seja $(\Omega_1, \mathcal{A}_1,P_1)$ um espaço de probabilidade e $(\Omega_2,\mathcal{A}_2)$ um espaço mensurável. Para cada $\omega_1 \in \Omega_1,$ defina uma medida de probabilidade $\mu(\omega_1,.)$ em $\mathcal{A}_2.$ Assuma também que, para cada $B \in \mathcal{A}_2,$ $\mu(.,B)$ é $\mathcal{A}_1$-mensurável. Então, existe uma única medida de probabilidade $P$ em $\mathcal{A}= \mathcal{A}_1\times\mathcal{A}_2$ tal que 

$P(A\times B)$ $=\displaystyle\int_A \mu(\omega_1,B)dP_1(\omega_1)~,~$ $\forall A\in \mathcal{A}_1,$ $\forall B\in \mathcal{A}_2.$

$~$

Se $D(\omega_1)$ denota uma secção de $D$ em $\omega_1,$ isto é, $D(\omega_1)=$ $\{\omega_2\in \Omega_2: (\omega_1,\omega_2)\in D\},$ $D\in \mathcal{A}=\mathcal{A}_1\times\mathcal{A}_2,$ então $P(D)$ $=\displaystyle\int_{\Omega_1}\mu\left(\omega_1,D(\omega_1)\right)dP_1(\omega_1).$

$~$

$~$

Voltando à probabilidade condicional, interprete (informalmente, por enquanto) a medida $\mu(x,B)$ do teorema anterior como $P(Y\in B| X=x).$ Ainda informalmente, considere o evento $\{X=x\}$. Intuitivamente, a probabilidade que $X\in (x,x+dx]$ é $dF_X(x).$ 
Então, sabendo que $X=x$ ocorreu, o evento $\left\{(X,Y)\in C\right\}$ ocorre se, e somente, $Y \in C(x)$ $=\{y:(x,y)\in C\}$ e a probabilidade desse evento é $\mu(x,C(x)).$ Pela regra da probabilidade total,

$P(C)$ $=P\left(\left\{(X,Y)\in C\right\}\right)$ $=\displaystyle\int_{\mathbb{R}}\mu\left(x,C(x)\right)dF(x).$

Em particular, quando $C=\{(x,y):~ x\in A, y \in B\}$ $=A\times B~,$ $C(x)=B$ se $x\in A$ e $C(x)=\varnothing$ se $x \notin A~,$ então

$P(C)$ $=P(A\times B)$ $=\displaystyle\int_A \mu(x,B)dF(x)$

Se $\mu(x,B)$ é mensurável em $x$ para cada $B\in \mathcal{B}(\mathbb{R}),$ então, pelo Teorema anterior, $P$ é única.

$~$

> **Exemplo:** Seja $X \sim Beta(a,b)$ e $Y|X=x \sim Bin(n,x)$  
$~$  
Considere $\left(\Omega_1=[0,1],\mathcal{A}_1=\mathcal{B}([0,1]),P_X\right),$ de modo que, para $A \in \mathcal{A}_1~,$    
$P_X(A)$  $=\displaystyle\int_{\mathbb{R}}\mathbb{I}_A dF_X(x)$ $=\displaystyle\int_A f_X(x)dx$ $=\displaystyle\int_A \tfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}~x^{a-1}(1-x)^{b-1}dx$.  
Além disso, considere $\left(\Omega_2=\{0,1,\ldots,n\}, \mathcal{A}_2=\mathcal{P}(\Omega_2)\right)$ e, para cada $x \in [0,1]~,$ defina $\mu(x,B)=P(Y \in B~|~ X=x).$ Então, para $k=0,1,\ldots,n,$  
$\mu\left(x,\{k\}\right)$ $=P(Y=k~|~ X=x)$ $=\displaystyle\binom{n}{k}x^k(1-x)^{n-k}~$ (que é mensurável em $x$).  
Tomando $\Omega=\Omega_1 \times \Omega_2~,~$ $\mathcal{A}=\mathcal{A}_1 \times \mathcal{A}_2~,~$ $P$ é a única medida de probabilidade determinada por $P_X$ (ou $F_X$) e $\mu(x,\cdot)~.$ Assim, para $C \in \mathcal{A}~,$  
$P(C)$ $=\displaystyle\int_{\Omega_1}\mu\left(x,C(x)\right)dP_X$ $=\displaystyle\int_0^1 \mu\left(x,C(x)\right)dF_X(x)$ $=\displaystyle\int_0^1 \mu\left(x,C(x)\right)f_X(x)dx~.$  
$~$  
Por exemplo, se $C=\Omega_1 \times \{k\},$ temos  
$P\left(\Omega_1 \times \{k\}\right)$
$=P\left(\left\{X\in[0,1]~,~Y=k\right\}\right)$ 
$=P_Y\left(Y=k\right)$
$=\displaystyle\int_0^1P(Y=k|X=x)dF_X(x)$ 
$=\displaystyle\int_0^1P(Y=k|X=x)f_X(x)dx$ 
$=\displaystyle\int_0^1 \binom{n}{k}x^k(1-x)^{n-k} ~\tfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}~x^{a-1}(1-x)^{b-1}dx$ 
$=\displaystyle\binom{n}{k}\tfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\tfrac{\Gamma(a+k)\Gamma(b+n-k)}{\Gamma(a+b+n)}\int_0^1\tfrac{\Gamma(a+b+n)}{\Gamma(a+k)\Gamma(b+n-k)}~x^{(a+k)-1}(1-x)^{(b+n-k)-1}~dx$ 
$=\displaystyle\binom{n}{k}\tfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\tfrac{\Gamma(a+k)\Gamma(b+n-k)}{\Gamma(a+b+n)}$
$=\displaystyle\binom{n}{k}\dfrac{\beta(a+k,b+n-k)}{\beta(a,b)}~.$  
$~$  
Nesse caso, diz-se que $Y \sim \text{Beta-Bin}(n,a,b)$.  

$~$

$~$

**Teorema** Considere $(\Omega,\mathcal{A},P)$ e $X:\Omega \longrightarrow \mathfrak{X}$, $\mathcal{F}$ uma $\sigma$-álgebra de $\mathfrak{X}$ e $B \in \mathcal{A}.$ Então existe $g:\mathfrak{X} \longrightarrow \mathbb{R}$ tal que, para cada $A \in \mathcal{F},$ 
$P(\{X\in A\}\cap B)$ $=\displaystyle\int_Ag(x)dP_X(x).$  
Além disso, $g$ é única $[P_X]$ q.c., isto é, $g(x)=P(B|X=x)$ é única $[P_X]$ q.c. para um dado $B\in\mathcal{A}$.

> **Demo:** segue diretamente do Teorema de Radon-Nikodin: se $\mu(A)=$ $P(\{X\in A\}\cap B)$ então $\mu$ é medida finita em $\mathcal{F}$ com $\mu << P_X$.

$~$ 

> **Exemplo 1.** Seja $\mathfrak{X}=\{x_1,x_2,\ldots\}$ com $p_i=P(\{X=x_i\})>0.$ Para $i=1,2,\ldots$, considere a função $g$, uma "proposta" para $P\left(B|\{X=x_i\}\right)$, definida por $g(x_i)$ $=P\left(B|\{X=x_i\}\right)$ $=\dfrac{P\left(B\cap \{X=x_i\}\right)}{P\left(\{X=x_i\}\right)}~.$  
Seja $A \in \mathcal{F}=\mathcal{P}(\mathfrak{X}),$ então  
$\displaystyle\int_A g(x)~dP_X(x)$ 
$=\displaystyle\int_{\mathfrak{X}}g(x)~\mathbb{I}_A(x)dP_X(x)$ 
$=\displaystyle\sum_{i=1}^\infty g(x_i)~\mathbb{I}_A(x_i)P_X(X=x_i)$ 
$=\displaystyle\sum_{x_i \in A}g(x_i)P\left(\{X=x_i\}\right)$ 
$=\displaystyle\sum_{x_i \in A}\dfrac{P\left(B\cap \{X=x_i\}\right)}{P\left(\{X=x_i\}\right)}P\left(\{X=x_i\}\right)$ 
$=\displaystyle\sum_{x_i \in A}P\left(B \cap \{X=x_i\}\right)$ 
$=P\left(\{X\in A\}\cap B\right).$

$~$

>**Exemplo 2.** Considere agora $\Omega=\mathbb{R}^2,$ $\mathcal{A}= \mathcal{B}\left(\mathbb{R}^2\right),$ $X(x,y)=x,$ $Y(x,y)=y~.$ e $\left(X,Y\right)$ vetor aleatório (absolutamente) contínuo com *densidade conjunta* $f$, isto é, $P(A)=\displaystyle\int \int_A f(x,y)~dxdy~,~~$ $A \in \mathcal{A}~.$ Nesse caso $P\left(\{X=x\}\right)=0~,~~ \forall x~.$  
$~$  
Seja $f_1(x)=\displaystyle\int_{-\infty}^{\infty} f(x,y)~dy$ a *densidade marginal* de $X$ e defina $f(y|x)=\dfrac{f(x,y)}{f_1(x)}$ como a *densidade condicional* de $Y$ dado $X=x.$  
Note que $f(y|x)$ só está definido quando $f_1(x) \neq 0.$  Contudo, se $S=\{(x,y): f_1(x)=0\}$ então  
$P\left(\{(X,Y)\in S\}\right)$ 
$=\displaystyle\int \int_S f(x,y)dxdy$ $=\displaystyle\int_{\{x:f_1(x)=0\}}\left[\int_{-\infty}^\infty f(x,y)dy\right]dx$ 
$=\displaystyle\int_{\{x:f_1(x)=0\}} f_1(x)dx=0~,$
de modo que $P\left(\{(X,Y)\in S\}\right)=0$ e podemos "ignorar" o conjunto onde $f(y|x)$ não está definida.  
$~$  
Se $X=x,$ $\forall~ B \in \mathcal{A},$ $B$ ocorre se, e somente se, $Y \in B(x)=\left\{y:(x,y) \in B\right\}.$ Assim, considere a "proposta"  
$g(x)$ $=P\left(\left\{Y \in B(x)|X=x\right\}\right)$ 
$=\displaystyle\int_{B(x)}f(y|x)dy$ 
$=\displaystyle\int_{-\infty}^\infty \mathbb{I}_B(x,y)f(y|x)dy~.$   
$~$  
Então, se $A \in \mathcal{B}(\mathbb{R}),$  
$P\left(\{X \in A\}\cap B\right)$ 
$=\displaystyle\underset{\left\{x\in A~;~(x,y)\in B\right\}}{\int\int} f(x,y)dxdy$ 
$=\displaystyle\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}\mathbb{I}_B(x,y)f(y|x)dy\right]~\mathbb{I}_A(x)f_1(x)dx$ 
$=\displaystyle\int_Af_1(x)dx\underbrace{\int_{B(x)}f(y|x)dy}_{g(x)}dx$ 
$=\displaystyle\int_Ag(x)f_1(x)dx$ 
$=\displaystyle\int_Ag(x)dP_X(x)~.$   
Portanto, $g(x)=P(B|X=x)~.$


$P(X\in A,Y \in B)=$ $\int_A \int_Bf(x,y)dxdy=$ $\int_A \int_Bf(y|x)f_1(x)dydx=$ $\int_B \int_Af(x|y)f_2(y)dxdy$


**Exemplo**

$(\Omega=[0,1]^2, \mathcal{A}=\mathcal{B}([0,1]^2),P=\lambda)$

$X(\boldsymbol \omega)=\left\{\begin{array}{lll}
x_1, & \omega_1 \leq 1/2 & (A_1)\\
x_2, & \omega_2 > 1/2 & (A_2)\end{array}\right.$

$Y(\boldsymbol \omega)=\left\{\begin{array}{lll}
y_1, & \omega_1 \leq \omega_2 & (B_1)\\
y_2, & \omega_1 > \omega_2& (B_2)\end{array}\right.$

IMAGEM DAS PARTIÇÕES

$P_X(x_1)=$ $P(X^{-1}(\{x_1\}))=$ $P(\boldsymbol \omega \in A_1)=$ $\lambda(A_1)=1/2$

$P_Y(y_1)=$ $P(Y^{-1}(\{y_1\}))=$ $P(\boldsymbol \omega \in B_1)=$ $\lambda(B_1)=1/2$

$\sigma_X =$ $\{\varnothing,A_1,A_2,\Omega\} \subseteq \mathcal{B}([0,1]^2)$ (é sub-$\sigma$-álgebra)

$\sigma_Y =$ $\{\varnothing,B_1,B_2,\Omega\} \subseteq \mathcal{B}([0,1]^2)$

Seja $\boldsymbol Z(\boldsymbol \omega)=$ $(X(\boldsymbol \omega), Y(\boldsymbol \omega))=$ $(X,Y)(\boldsymbol \omega),$ $Z: \Omega\longrightarrow \mathbb{R}^2$ $Z(\boldsymbol \omega)=$ $\sum_{i=1}^4 \boldsymbol z_i ~\mathbb{I}_{C_i}(\boldsymbol \omega)$ é função simples.

$Z(\boldsymbol \omega)=\left\{\begin{array}{ll}
(x_1,y_1)=z_1, & \boldsymbol \omega \in A_1 \cap B_1=C_1\\
(x_1,y_2)=z_2, & \boldsymbol \omega \in A_1 \cap B_2=C_2\\
(x_2,y_1)=z_3, & \boldsymbol \omega \in A_2 \cap B_1=C_3\\
(x_2,y_2)=z_4, & \boldsymbol \omega \in A_2 \cap B_2=C_4
\end{array}\right.$

$P_Z((\underbrace{x_1,y_2}_{z_2}))=$ $P_Z((\underbrace{x_2,y_1}_{z_3}))=$ $\dfrac{1}{8}=$ $\lambda(\underbrace{A_1\cap B_2}_{C_2})=$ $\lambda(\underbrace{A_2\cap B_1}_{C_3})$ 

$P_Z((\underbrace{x_1,y_1}_{z_1}))=$ $P_Z((\underbrace{x_2,y_2}_{z_4}))=$ $\dfrac{3}{8}=$ $\lambda(\underbrace{A_1\cap B_1}_{C_1})=$ $\lambda(\underbrace{A_2\cap B_2}_{C_4})$ 

$P_Z(\boldsymbol z_1| \boldsymbol z_1 \cap \boldsymbol z_3)=$ $\dfrac{P_Z(\boldsymbol z_1 \cap (\boldsymbol z_1 \cup \boldsymbol  z_3))}{P_Z(\boldsymbol  z_1 \cup \boldsymbol z_3)}=$ $\dfrac{P_Z(\boldsymbol  z_1)}{P_Z(\boldsymbol  z_1)+P_Z(\boldsymbol  z_3)}=$ $\dfrac{3/8}{3/8 + 1/8}=$ $\dfrac{3}{4}=$ $P_Z((X=x_1,Y=y_1)|\overbrace{X \in \{x_1,x_2\}}^{\Omega}, Y=y_1)=$ $P_{X|Y=y_1}(X=x_1|Y=y_1)=$ $1-P_{X|y_1}(X=x_2|Y=y_1).$

$P_{X|y_1}(X=x_1|Y=y_2)= \dfrac{1/8}{4/8}=\dfrac{1}{4}$

Pela aula passada, podeos calcular $E[X|Y=y_1]$ como $E[X|Y=y_1]=$ $\int x dP_{X|Y=y_1}(x)=$ $\sum_{i=1}^2x_i P_{X|Y=y_1}(x_i|y_1)$

Por exemplo, se $x_1=y_1=1,$ $x_2=y_2=2,$ temos $E[X|Y=1]=$ $1*\dfrac{3}{4}+2*\dfrac{1}{4}=\dfrac{5}{4}$

Analogamente,

$E[X|Y=2]=$ $1*\dfrac{1}{4}+2*\dfrac{3}{4}=\dfrac{7}{4}$

$E[X|Y](\omega)=\left\{\begin{array}{ll}
5/4, & \omega \in B_1\\
7/4, & \omega \in B_2
\end{array}\right.$

$E[X|Y]=E[X|\sigma_X].$
